{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (49_999, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentence</th><th>label</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;刚开始是牙炎痛后面又口腔溃疡&quot;</td><td>&quot;牙齿炎症治疗&quot;</td></tr><tr><td>&quot;我想问一下整个过程大概要多久&quot;</td><td>&quot;无主题&quot;</td></tr><tr><td>&quot;就是我想质询智齿&quot;</td><td>&quot;智齿&quot;</td></tr><tr><td>&quot;请问免疫抑制剂对口腔溃疡恢复有什么作用呢？&quot;</td><td>&quot;口腔溃疡&quot;</td></tr><tr><td>&quot;补牙是要么收费的&quot;</td><td>&quot;补牙&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;有点儿敏感&quot;</td><td>&quot;牙齿脱敏&quot;</td></tr><tr><td>&quot;没有呢？今天才痛的&quot;</td><td>&quot;牙齿疼痛&quot;</td></tr><tr><td>&quot;以前最早是补了? 后来医生建议拔掉? 因为最后一颗牙 没什么…</td><td>&quot;拔牙&quot;</td></tr><tr><td>&quot;下牙，从里面数出来第2颗，右边&quot;</td><td>&quot;无主题&quot;</td></tr><tr><td>&quot;有什么办法止疼吗？&quot;</td><td>&quot;牙齿疼痛&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (49_999, 2)\n",
       "┌─────────────────────────────────┬──────────────┐\n",
       "│ sentence                        ┆ label        │\n",
       "│ ---                             ┆ ---          │\n",
       "│ str                             ┆ str          │\n",
       "╞═════════════════════════════════╪══════════════╡\n",
       "│ 刚开始是牙炎痛后面又口腔溃疡    ┆ 牙齿炎症治疗 │\n",
       "│ 我想问一下整个过程大概要多久    ┆ 无主题       │\n",
       "│ 就是我想质询智齿                ┆ 智齿         │\n",
       "│ 请问免疫抑制剂对口腔溃疡恢复有  ┆ 口腔溃疡     │\n",
       "│ 什么作用呢？                    ┆              │\n",
       "│ 补牙是要么收费的                ┆ 补牙         │\n",
       "│ …                               ┆ …            │\n",
       "│ 有点儿敏感                      ┆ 牙齿脱敏     │\n",
       "│ 没有呢？今天才痛的              ┆ 牙齿疼痛     │\n",
       "│ 以前最早是补了?                 ┆ 拔牙         │\n",
       "│ 后来医生建议拔掉?               ┆              │\n",
       "│ 因为最后一颗牙 没什么…          ┆              │\n",
       "│ 下牙，从里面数出来第2颗，右边   ┆ 无主题       │\n",
       "│ 有什么办法止疼吗？              ┆ 牙齿疼痛     │\n",
       "└─────────────────────────────────┴──────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pl.read_csv('data/train.csv')\n",
    "data_train = data_train.drop_nulls()\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepath):\n",
    "    data = pl.read_csv(filepath)\n",
    "    data = data.drop_nulls()\n",
    "    item2id, id2item = {}, {}\n",
    "    for i, label in enumerate(data['label'].value_counts().sort('count', descending=True)['label']):\n",
    "        item2id[label] = i\n",
    "        id2item[i] = label\n",
    "    data = data.with_columns(data[\"label\"].replace(item2id).cast(pl.Int64).alias(\"label\"))\n",
    "    return data, item2id, id2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (24, 2)\n",
      "┌──────────┬───────┐\n",
      "│ label    ┆ count │\n",
      "│ ---      ┆ ---   │\n",
      "│ str      ┆ u32   │\n",
      "╞══════════╪═══════╡\n",
      "│ 无主题   ┆ 15257 │\n",
      "│ 拔牙     ┆ 3806  │\n",
      "│ 补牙     ┆ 3644  │\n",
      "│ 牙齿矫正 ┆ 3380  │\n",
      "│ 牙齿疼痛 ┆ 3304  │\n",
      "│ …        ┆ …     │\n",
      "│ 牙齿脱敏 ┆ 624   │\n",
      "│ 牙齿美白 ┆ 575   │\n",
      "│ 窝沟封闭 ┆ 111   │\n",
      "│ 美牙冠   ┆ 90    │\n",
      "│ 涂氟防龋 ┆ 63    │\n",
      "└──────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "data = pl.read_csv('data/train.csv')\n",
    "data = data.drop_nulls()\n",
    "item2id, id2item = {}, {}\n",
    "for i, label in enumerate(data['label'].value_counts().sort('count', descending=True)['label']):\n",
    "    item2id[label] = i\n",
    "    id2item[i] = label\n",
    "print(data['label'].value_counts().sort('count', descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigorange/miniconda3/envs/huggingface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        super().__init__()\n",
    "        self.data, self.item2id, self.id2item = data_process(filepath)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.row(index)\n",
    "        return row[0], row[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_data = MyDataset('data/train.csv')\n",
    "test_data = MyDataset('data/test.csv')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = [], []\n",
    "    for item in batch:\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "    inputs = tokenizer(texts, padding = True, return_tensors = 'pt')\n",
    "    inputs['labels'] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('刚开始是牙炎痛后面又口腔溃疡', 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'input_ids': tensor([[ 101, 2769, 2682,  ...,    0,    0,    0],\n",
       "         [ 101, 1486, 6418,  ...,    0,    0,    0],\n",
       "         [ 101,  872,  812,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 4385, 1762,  ...,    0,    0,    0],\n",
       "         [ 101, 3175,  912,  ...,    0,    0,    0],\n",
       "         [ 101,  671, 5663,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([ 3,  8,  7, 16, 13,  0, 11,  0,  0,  6,  3,  3,  0,  0,  8, 18,  5, 11,\n",
       "         12,  7, 11,  7, 16,  6,  1,  0,  0,  9, 14, 11,  0,  3, 16,  6,  0,  0,\n",
       "          0, 19,  6,  0,  6,  6,  5,  4,  1,  2,  7,  1,  1,  0,  5, 12,  0, 10,\n",
       "          6,  0,  4, 20, 20,  0, 18,  4,  0,  0])})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=len(train_data.item2id))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "else:\n",
    "        device = 'cpu'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(3.6318, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.6133, -0.1696, -0.5780,  ...,  0.4765,  0.4000,  0.3436],\n",
       "        [-0.6161, -0.4710, -0.9286,  ...,  0.4833,  0.1897,  0.1197],\n",
       "        [-0.8817, -0.3529, -0.5881,  ...,  0.5664, -0.1477,  0.0103],\n",
       "        ...,\n",
       "        [-0.4782, -0.1858, -0.6568,  ...,  0.5218,  0.5450,  0.2358],\n",
       "        [-1.2511, -0.6045, -1.1756,  ...,  0.6348, -0.0875, -0.2760],\n",
       "        [-0.3957, -0.0640, -0.6537,  ...,  0.4307,  0.4840,  0.5615]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**next(enumerate(trainloader))[1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, global_step: 0, loss: 3.720447063446045\n",
      "epoch: 0, global_step: 100, loss: 1.6409595012664795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, global_step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m             global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, log_step)\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:820\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:821\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 821\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch=1, log_step=100):\n",
    "    \n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0:\n",
    "                print(f'epoch: {ep}, global_step: {global_step}, loss: {output.loss.item()}')\n",
    "            global_step += 1\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
